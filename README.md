# üß™ Proyecto: Pipeline de Datos Escalable y Confiable

## üéØ Objetivo

Dise√±ar un pipeline ETL usando Spark y Airflow para procesar datos de ventas diarias desde archivos CSV, aplicando limpieza, transformaci√≥n y carga final en una base de datos relacional.

---

## üåê Repositorio del Proyecto

Este proyecto est√° alojado en GitHub:

> üîó https://github.com/tu_usuario/t_r_transaction_pao

> *(Reemplaza con tu URL real si a√∫n no lo has subido)*

---

## ‚öôÔ∏è Tecnolog√≠as Usadas

- **Apache Airflow**: Orquestaci√≥n del pipeline.
- **Apache Spark (PySpark)**: Procesamiento de datos batch.
- **MySQL**: Base de datos simulada (contenedor).
- **Docker & Docker Compose**: Entorno reproducible.
- **Python 3.10**: Scripts de procesamiento.

---

## üìÇ Estructura del Proyecto

```
t_r_transaction_pao/
‚îÇ
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ pipeline_dag.py             # DAG principal de Airflow
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ ingestion.py                # Ingesta de CSV a Spark
‚îÇ   ‚îú‚îÄ‚îÄ cleaning.py                 # Limpieza de datos
‚îÇ   ‚îú‚îÄ‚îÄ transformation.py          # Transformaciones (agregaci√≥n de ventas)
‚îÇ   ‚îî‚îÄ‚îÄ load.py                     # Carga final en base de datos
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ Ventas_diarias.csv          # Datos fuente
‚îÇ   ‚îî‚îÄ‚îÄ intermediate_*.parquet      # Datos procesados en etapas
‚îÇ
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml
‚îÇ
‚îî‚îÄ‚îÄ requirements.txt
```

---

## üöÄ C√≥mo Ejecutar el Proyecto

### 1. Clona este repositorio

```bash
git clone https://github.com/tu_usuario/t_r_transaction_pao.git
cd t_r_transaction_pao/docker
```

### 2. Levantar el entorno con Docker

```bash
docker-compose up --build airflow-webserver airflow-scheduler
```

> üí° Alternativamente:

```bash
docker-compose up --build
docker-compose up -d airflow-scheduler
```

### 3. Abrir Airflow

- URL: [http://localhost:8080](http://localhost:8080)
- Usuario: `admin`
- Contrase√±a: `admin`

---

## üìà Descripci√≥n del Pipeline

| Etapa | Descripci√≥n |
|-------|-------------|
| **Ingesta** | Lee el archivo `Ventas_diarias.csv` usando Spark y lo guarda como Parquet. |
| **Limpieza** | Elimina valores nulos, corrige formatos de fecha y columnas inconsistentes. |
| **Transformaci√≥n** | Suma ventas por d√≠a usando `groupBy` y `sum`. |
| **Carga** | Simula la carga en una base de datos relacional (MySQL contenedor). |

---

## üìä Resultados de Ejecuci√≥n

- **Registros ingeridos:** Depende del archivo CSV (`Ventas_diarias.csv`)
- **Registros luego de limpieza:** Filtrados por datos nulos o inconsistentes
- **Registros transformados:** Totales de ventas por d√≠a
- **Logs:** Visibles en la interfaz de Airflow

---

## üõ†Ô∏è Mejoras Futuras

- Agregar alertas autom√°ticas por email/Slack (simuladas o reales)
- Validaciones estad√≠sticas y checks de calidad de datos
- Escalar hacia procesamiento distribuido real en cl√∫ster
- Pruebas automatizadas de los m√≥dulos Spark

---

## üì¶ Nota sobre archivos generados autom√°ticamente

Este repositorio omite ciertos archivos como:

- Archivos `.parquet` intermedios generados por el pipeline
- Carpetas `__pycache__/` de Python
- Directorios `.idea/` del entorno de desarrollo

Estos archivos se regeneran autom√°ticamente al ejecutar el DAG `pipeline_datos`, comenzando desde el archivo fuente `Ventas_diarias.csv`.  
Esto mantiene el repositorio limpio, portable y alineado con buenas pr√°cticas de desarrollo.
