
# ğŸ§ª Proyecto: Pipeline de Datos Escalable y Confiable

## ğŸ¯ Objetivo

DiseÃ±ar un pipeline ETL usando Spark y Airflow para procesar datos de ventas diarias desde archivos CSV, aplicando limpieza, transformaciÃ³n y carga final en una base de datos relacional.

---

## ğŸŒ Repositorio del Proyecto

Este proyecto estÃ¡ alojado en GitHub:

> ğŸ”— https://github.com/tu_usuario/t_r_transaction_pao

> *(Reemplaza con tu URL real si aÃºn no lo has subido)*

---

## âš™ï¸ TecnologÃ­as Usadas

- **Apache Airflow**: OrquestaciÃ³n del pipeline.
- **Apache Spark (PySpark)**: Procesamiento de datos batch.
- **MySQL**: Base de datos simulada (contenedor).
- **Docker & Docker Compose**: Entorno reproducible.
- **Python 3.10**: Scripts de procesamiento.

---

## ğŸ“‚ Estructura del Proyecto

```
t_r_transaction_pao/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ pipeline_dag.py             # DAG principal de Airflow
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingestion.py                # Ingesta de CSV a Spark
â”‚   â”œâ”€â”€ cleaning.py                 # Limpieza de datos
â”‚   â”œâ”€â”€ transformation.py          # Transformaciones (agregaciÃ³n de ventas)
â”‚   â””â”€â”€ load.py                     # Carga final en base de datos
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Ventas_diarias.csv          # Datos fuente
â”‚   â””â”€â”€ intermediate_*.parquet      # Datos procesados en etapas
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â””â”€â”€ requirements.txt
```

---

## ğŸš€ CÃ³mo Ejecutar el Proyecto

### 1. Clona este repositorio

```bash
git clone https://github.com/tu_usuario/t_r_transaction_pao.git
cd t_r_transaction_pao/docker
```

### 2. Levantar el entorno con Docker

```bash
docker-compose up --build airflow-webserver airflow-scheduler
```

> ğŸ’¡ Alternativamente:

```bash
docker-compose up --build
docker-compose up -d airflow-scheduler
```

### 3. Abrir Airflow

- URL: [http://localhost:8080](http://localhost:8080)
- Usuario: `admin`
- ContraseÃ±a: `admin`

---

## ğŸ“ˆ DescripciÃ³n del Pipeline

| Etapa | DescripciÃ³n |
|-------|-------------|
| **Ingesta** | Lee el archivo `Ventas_diarias.csv` usando Spark y lo guarda como Parquet. |
| **Limpieza** | Elimina valores nulos, corrige formatos de fecha y columnas inconsistentes. |
| **TransformaciÃ³n** | Suma ventas por dÃ­a usando `groupBy` y `sum`. |
| **Carga** | Simula la carga en una base de datos relacional (MySQL contenedor). |

---

## ğŸ“Š Resultados de EjecuciÃ³n

- **Registros ingeridos:** Depende del archivo CSV (`Ventas_diarias.csv`)
- **Registros luego de limpieza:** Filtrados por datos nulos o inconsistentes
- **Registros transformados:** Totales de ventas por dÃ­a
- **Logs:** Visibles en la interfaz de Airflow

---

## ğŸ› ï¸ Mejoras Futuras

- Agregar alertas automÃ¡ticas por email/Slack (simuladas o reales)
- Validaciones estadÃ­sticas y checks de calidad de datos
- Escalar hacia procesamiento distribuido real en clÃºster
- Pruebas automatizadas de los mÃ³dulos Spark

