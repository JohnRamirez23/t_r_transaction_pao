
# И Proyecto: Pipeline de Datos Escalable y Confiable

##  Objetivo

Dise帽ar un pipeline ETL usando Spark y Airflow para procesar datos de ventas diarias desde archivos CSV, aplicando limpieza, transformaci贸n y carga final en una base de datos relacional.

---

##  Repositorio del Proyecto

Este proyecto est谩 alojado en GitHub:

>  https://github.com/JohnRamirez23/t_r_transaction_pao

---

## 锔 Tecnolog铆as Usadas

- **Apache Airflow**: Orquestaci贸n del pipeline.
- **Apache Spark (PySpark)**: Procesamiento de datos batch.
- **MySQL**: Base de datos simulada (contenedor).
- **Docker & Docker Compose**: Entorno reproducible.
- **Python 3.10**: Scripts de procesamiento.

---

##  Estructura del Proyecto

```
t_r_transaction_pao/
 dags/
 scripts/
 data/
 docker/
 evidencia_ejecucion/
 requirements.txt
 README.md
```

---

##  C贸mo Ejecutar el Proyecto

### 1. Clona este repositorio

```bash
git clone https://github.com/JohnRamirez23/t_r_transaction_pao.git
cd t_r_transaction_pao/docker
```

### 2. Levantar el entorno con Docker

```bash
docker-compose up --build airflow-webserver airflow-scheduler
```

>  Alternativamente:

```bash
docker-compose up --build
docker-compose up -d airflow-scheduler
```

### 3. Abrir Airflow

- URL: [http://localhost:8081](http://localhost:8081)
- Usuario: `admin`
- Contrase帽a: `admin`

---

##  Descripci贸n del Pipeline

| Etapa | Descripci贸n |
|-------|-------------|
| **Ingesta** | Lee el archivo `Ventas_diarias.csv` usando Spark y lo guarda como Parquet. |
| **Limpieza** | Elimina valores nulos, corrige formatos de fecha y columnas inconsistentes. |
| **Transformaci贸n** | Suma ventas por d铆a usando `groupBy` y `sum`. |
| **Carga** | Simula la carga en una base de datos relacional (MySQL contenedor). |

---

##  Resultados de Ejecuci贸n

- **Registros ingeridos:** Dependen del archivo CSV (`Ventas_diarias.csv`)
- **Registros luego de limpieza:** Filtrados por datos nulos o inconsistentes
- **Registros transformados:** Totales de ventas por d铆a
- **Logs:** Visibles en Airflow y almacenados en MySQL

---

##  Evidencia de Ejecuci贸n

### DAG ejecutado en Airflow

La siguiente imagen muestra el DAG `pipeline_datos` orquestado correctamente, con todas las tareas completadas con 茅xito (`success`):

![DAG ejecutado](evidencia_ejecucion/dag_pipeline.png)

### Logs desde MySQL

Tambi茅n se incluye una captura de los registros de ejecuci贸n guardados en la base de datos `ventas_db` (tabla `log`), que muestran las tareas realizadas por Airflow:

![Logs desde MySQL](evidencia_ejecucion/logs_mysql_workbench.png)

---

##  Nota sobre archivos generados autom谩ticamente

Este repositorio omite ciertos archivos como:

- Archivos `.parquet` intermedios generados por el pipeline
- Carpetas `__pycache__/` de Python
- Directorios `.idea/` del entorno de desarrollo

Estos archivos se regeneran autom谩ticamente al ejecutar el DAG `pipeline_datos`, comenzando desde el archivo fuente `Ventas_diarias.csv`.  
Esto mantiene el repositorio limpio, portable y alineado con buenas pr谩cticas de desarrollo.
